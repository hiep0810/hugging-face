{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiepdvh/miniconda3/envs/huggingface/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_dataset = load_dataset('nyu-mll/glue', 'mrpc')\n",
    "\n",
    "checkpoint = 'google-bert/bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['sentence1'],\n",
    "        examples['sentence2'],\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label', 'labels')\n",
    "tokenized_dataset = tokenized_dataset.with_format('torch')\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "val_dataloder = DataLoader(\n",
    "    tokenized_dataset['validation'],\n",
    "    batch_size=64,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': torch.Size([64]), 'input_ids': torch.Size([64, 90]), 'token_type_ids': torch.Size([64, 90]), 'attention_mask': torch.Size([64, 90])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "print({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6113, grad_fn=<NllLossBackward0>) torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'bert-base-cased'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/290. Loss: 0.4184406101703644\n",
      "2/290. Loss: 0.4247891902923584\n",
      "3/290. Loss: 0.5021761059761047\n",
      "4/290. Loss: 0.4245592951774597\n",
      "5/290. Loss: 0.3845520317554474\n",
      "6/290. Loss: 0.3684302866458893\n",
      "7/290. Loss: 0.47829633951187134\n",
      "8/290. Loss: 0.4160231649875641\n",
      "9/290. Loss: 0.39048781991004944\n",
      "10/290. Loss: 0.42099347710609436\n",
      "11/290. Loss: 0.3905502259731293\n",
      "12/290. Loss: 0.3801862597465515\n",
      "13/290. Loss: 0.4951932430267334\n",
      "14/290. Loss: 0.48998838663101196\n",
      "15/290. Loss: 0.36677631735801697\n",
      "16/290. Loss: 0.2566084861755371\n",
      "17/290. Loss: 0.40626609325408936\n",
      "18/290. Loss: 0.33837369084358215\n",
      "19/290. Loss: 0.34840187430381775\n",
      "20/290. Loss: 0.37006378173828125\n",
      "21/290. Loss: 0.3054410219192505\n",
      "22/290. Loss: 0.3960416615009308\n",
      "23/290. Loss: 0.27872946858406067\n",
      "24/290. Loss: 0.312818706035614\n",
      "25/290. Loss: 0.3463079631328583\n",
      "26/290. Loss: 0.26707780361175537\n",
      "27/290. Loss: 0.3592855930328369\n",
      "28/290. Loss: 0.31880733370780945\n",
      "29/290. Loss: 0.34505394101142883\n",
      "30/290. Loss: 0.29686683416366577\n",
      "31/290. Loss: 0.3683246970176697\n",
      "32/290. Loss: 0.31486591696739197\n",
      "33/290. Loss: 0.3206768035888672\n",
      "34/290. Loss: 0.2691861093044281\n",
      "35/290. Loss: 0.2607317268848419\n",
      "36/290. Loss: 0.20659948885440826\n",
      "37/290. Loss: 0.3383614718914032\n",
      "38/290. Loss: 0.38220539689064026\n",
      "39/290. Loss: 0.621419370174408\n",
      "40/290. Loss: 0.24953237175941467\n",
      "41/290. Loss: 0.2783413529396057\n",
      "42/290. Loss: 0.21103906631469727\n",
      "43/290. Loss: 0.35435590147972107\n",
      "44/290. Loss: 0.23647761344909668\n",
      "45/290. Loss: 0.306398868560791\n",
      "46/290. Loss: 0.24490395188331604\n",
      "47/290. Loss: 0.3317386209964752\n",
      "48/290. Loss: 0.28441691398620605\n",
      "49/290. Loss: 0.3289206027984619\n",
      "50/290. Loss: 0.3008303642272949\n",
      "51/290. Loss: 0.3645346462726593\n",
      "52/290. Loss: 0.43419381976127625\n",
      "53/290. Loss: 0.26059627532958984\n",
      "54/290. Loss: 0.26994267106056213\n",
      "55/290. Loss: 0.4052835702896118\n",
      "56/290. Loss: 0.38073304295539856\n",
      "57/290. Loss: 0.36298617720603943\n",
      "58/290. Loss: 0.18259628117084503\n",
      "59/290. Loss: 0.22556617856025696\n",
      "60/290. Loss: 0.17717236280441284\n",
      "61/290. Loss: 0.09451033920049667\n",
      "62/290. Loss: 0.14554548263549805\n",
      "63/290. Loss: 0.21702495217323303\n",
      "64/290. Loss: 0.13784193992614746\n",
      "65/290. Loss: 0.11860770732164383\n",
      "66/290. Loss: 0.15092512965202332\n",
      "67/290. Loss: 0.13411739468574524\n",
      "68/290. Loss: 0.1729898750782013\n",
      "69/290. Loss: 0.06744913011789322\n",
      "70/290. Loss: 0.06467188149690628\n",
      "71/290. Loss: 0.13996121287345886\n",
      "72/290. Loss: 0.08058740198612213\n",
      "73/290. Loss: 0.10901645570993423\n",
      "74/290. Loss: 0.07686816155910492\n",
      "75/290. Loss: 0.08525492250919342\n",
      "76/290. Loss: 0.14216521382331848\n",
      "77/290. Loss: 0.07665137201547623\n",
      "78/290. Loss: 0.14948998391628265\n",
      "79/290. Loss: 0.05104310065507889\n",
      "80/290. Loss: 0.09549877792596817\n",
      "81/290. Loss: 0.1815609484910965\n",
      "82/290. Loss: 0.0706053078174591\n",
      "83/290. Loss: 0.10425909608602524\n",
      "84/290. Loss: 0.08645376563072205\n",
      "85/290. Loss: 0.18038831651210785\n",
      "86/290. Loss: 0.028867367655038834\n",
      "87/290. Loss: 0.25598812103271484\n",
      "88/290. Loss: 0.18325118720531464\n",
      "89/290. Loss: 0.2324329912662506\n",
      "90/290. Loss: 0.15277525782585144\n",
      "91/290. Loss: 0.1546999216079712\n",
      "92/290. Loss: 0.06980413943529129\n",
      "93/290. Loss: 0.11587801575660706\n",
      "94/290. Loss: 0.10444243252277374\n",
      "95/290. Loss: 0.22254569828510284\n",
      "96/290. Loss: 0.030483834445476532\n",
      "97/290. Loss: 0.11556003242731094\n",
      "98/290. Loss: 0.05335884168744087\n",
      "99/290. Loss: 0.15874168276786804\n",
      "100/290. Loss: 0.10560595989227295\n",
      "101/290. Loss: 0.16672062873840332\n",
      "102/290. Loss: 0.1327299177646637\n",
      "103/290. Loss: 0.147231787443161\n",
      "104/290. Loss: 0.06451652944087982\n",
      "105/290. Loss: 0.13164369761943817\n",
      "106/290. Loss: 0.11519620567560196\n",
      "107/290. Loss: 0.16557255387306213\n",
      "108/290. Loss: 0.19441711902618408\n",
      "109/290. Loss: 0.19110389053821564\n",
      "110/290. Loss: 0.16633197665214539\n",
      "111/290. Loss: 0.11174629628658295\n",
      "112/290. Loss: 0.12595908343791962\n",
      "113/290. Loss: 0.04993661493062973\n",
      "114/290. Loss: 0.06129860505461693\n",
      "115/290. Loss: 0.13316747546195984\n",
      "116/290. Loss: 0.12680760025978088\n",
      "117/290. Loss: 0.05796470120549202\n",
      "118/290. Loss: 0.055181387811899185\n",
      "119/290. Loss: 0.0380498543381691\n",
      "120/290. Loss: 0.04247758910059929\n",
      "121/290. Loss: 0.09257128089666367\n",
      "122/290. Loss: 0.07474371045827866\n",
      "123/290. Loss: 0.02911679446697235\n",
      "124/290. Loss: 0.035306401550769806\n",
      "125/290. Loss: 0.02013491280376911\n",
      "126/290. Loss: 0.024858534336090088\n",
      "127/290. Loss: 0.06698702275753021\n",
      "128/290. Loss: 0.11643966287374496\n",
      "129/290. Loss: 0.06392008811235428\n",
      "130/290. Loss: 0.0378585122525692\n",
      "131/290. Loss: 0.02216942049562931\n",
      "132/290. Loss: 0.12527228891849518\n",
      "133/290. Loss: 0.05375690013170242\n",
      "134/290. Loss: 0.06195870041847229\n",
      "135/290. Loss: 0.043214958161115646\n",
      "136/290. Loss: 0.05777783319354057\n",
      "137/290. Loss: 0.0239004734903574\n",
      "138/290. Loss: 0.14481206238269806\n",
      "139/290. Loss: 0.03696900233626366\n",
      "140/290. Loss: 0.021272027865052223\n",
      "141/290. Loss: 0.025038449093699455\n",
      "142/290. Loss: 0.08824694901704788\n",
      "143/290. Loss: 0.021214384585618973\n",
      "144/290. Loss: 0.15302139520645142\n",
      "145/290. Loss: 0.019433727487921715\n",
      "146/290. Loss: 0.033197563141584396\n",
      "147/290. Loss: 0.025830838829278946\n",
      "148/290. Loss: 0.07688646018505096\n",
      "149/290. Loss: 0.05992840230464935\n",
      "150/290. Loss: 0.02493312768638134\n",
      "151/290. Loss: 0.012284325435757637\n",
      "152/290. Loss: 0.016505440697073936\n",
      "153/290. Loss: 0.09896166622638702\n",
      "154/290. Loss: 0.011186044663190842\n",
      "155/290. Loss: 0.13740220665931702\n",
      "156/290. Loss: 0.025053249672055244\n",
      "157/290. Loss: 0.015840275213122368\n",
      "158/290. Loss: 0.027103552594780922\n",
      "159/290. Loss: 0.05247404798865318\n",
      "160/290. Loss: 0.009723561815917492\n",
      "161/290. Loss: 0.04455926641821861\n",
      "162/290. Loss: 0.011125118471682072\n",
      "163/290. Loss: 0.011658265255391598\n",
      "164/290. Loss: 0.009481427259743214\n",
      "165/290. Loss: 0.09948141872882843\n",
      "166/290. Loss: 0.012046962976455688\n",
      "167/290. Loss: 0.02900141105055809\n",
      "168/290. Loss: 0.03806145116686821\n",
      "169/290. Loss: 0.07270942628383636\n",
      "170/290. Loss: 0.008614137768745422\n",
      "171/290. Loss: 0.010697886347770691\n",
      "172/290. Loss: 0.05191753804683685\n",
      "173/290. Loss: 0.007557988166809082\n",
      "174/290. Loss: 0.034103043377399445\n",
      "175/290. Loss: 0.03830031678080559\n",
      "176/290. Loss: 0.006652548909187317\n",
      "177/290. Loss: 0.008737736381590366\n",
      "178/290. Loss: 0.01778237894177437\n",
      "179/290. Loss: 0.00733397901058197\n",
      "180/290. Loss: 0.02119094878435135\n",
      "181/290. Loss: 0.010583732277154922\n",
      "182/290. Loss: 0.025344790890812874\n",
      "183/290. Loss: 0.03192613273859024\n",
      "184/290. Loss: 0.008938595652580261\n",
      "185/290. Loss: 0.010611974634230137\n",
      "186/290. Loss: 0.008104824461042881\n",
      "187/290. Loss: 0.006429673638194799\n",
      "188/290. Loss: 0.020087450742721558\n",
      "189/290. Loss: 0.07774031162261963\n",
      "190/290. Loss: 0.008748253807425499\n",
      "191/290. Loss: 0.010880857706069946\n",
      "192/290. Loss: 0.06222812831401825\n",
      "193/290. Loss: 0.10794290155172348\n",
      "194/290. Loss: 0.018984289839863777\n",
      "195/290. Loss: 0.00984598696231842\n",
      "196/290. Loss: 0.008755848743021488\n",
      "197/290. Loss: 0.1348666548728943\n",
      "198/290. Loss: 0.00899692066013813\n",
      "199/290. Loss: 0.005906638689339161\n",
      "200/290. Loss: 0.0421471931040287\n",
      "201/290. Loss: 0.030341222882270813\n",
      "202/290. Loss: 0.009734564460814\n",
      "203/290. Loss: 0.00781710259616375\n",
      "204/290. Loss: 0.010291011072695255\n",
      "205/290. Loss: 0.008571857586503029\n",
      "206/290. Loss: 0.027064185589551926\n",
      "207/290. Loss: 0.008732863701879978\n",
      "208/290. Loss: 0.007824964821338654\n",
      "209/290. Loss: 0.013042928650975227\n",
      "210/290. Loss: 0.009587937034666538\n",
      "211/290. Loss: 0.006631811615079641\n",
      "212/290. Loss: 0.02069716528058052\n",
      "213/290. Loss: 0.01220616139471531\n",
      "214/290. Loss: 0.1074669137597084\n",
      "215/290. Loss: 0.008812577463686466\n",
      "216/290. Loss: 0.022304298356175423\n",
      "217/290. Loss: 0.011112285777926445\n",
      "218/290. Loss: 0.016597053036093712\n",
      "219/290. Loss: 0.0254436656832695\n",
      "220/290. Loss: 0.010815879330039024\n",
      "221/290. Loss: 0.013252356089651585\n",
      "222/290. Loss: 0.007903648540377617\n",
      "223/290. Loss: 0.016406923532485962\n",
      "224/290. Loss: 0.005977596156299114\n",
      "225/290. Loss: 0.013665984384715557\n",
      "226/290. Loss: 0.0056106275878846645\n",
      "227/290. Loss: 0.006492774933576584\n",
      "228/290. Loss: 0.012301729992032051\n",
      "229/290. Loss: 0.01905173808336258\n",
      "230/290. Loss: 0.005142810754477978\n",
      "231/290. Loss: 0.005309172440320253\n",
      "232/290. Loss: 0.014376387000083923\n",
      "233/290. Loss: 0.00672165397554636\n",
      "234/290. Loss: 0.03558465838432312\n",
      "235/290. Loss: 0.0071447682566940784\n",
      "236/290. Loss: 0.007544247899204493\n",
      "237/290. Loss: 0.06667011231184006\n",
      "238/290. Loss: 0.006131578236818314\n",
      "239/290. Loss: 0.019088363274931908\n",
      "240/290. Loss: 0.010851322673261166\n",
      "241/290. Loss: 0.007291450165212154\n",
      "242/290. Loss: 0.006380334496498108\n",
      "243/290. Loss: 0.005788279231637716\n",
      "244/290. Loss: 0.10338159650564194\n",
      "245/290. Loss: 0.00477040046826005\n",
      "246/290. Loss: 0.006584093440324068\n",
      "247/290. Loss: 0.01240367628633976\n",
      "248/290. Loss: 0.013672247529029846\n",
      "249/290. Loss: 0.0057646408677101135\n",
      "250/290. Loss: 0.05005559325218201\n",
      "251/290. Loss: 0.005027857609093189\n",
      "252/290. Loss: 0.01722102425992489\n",
      "253/290. Loss: 0.023193687200546265\n",
      "254/290. Loss: 0.005170466378331184\n",
      "255/290. Loss: 0.005815984681248665\n",
      "256/290. Loss: 0.008866996504366398\n",
      "257/290. Loss: 0.004884035792201757\n",
      "258/290. Loss: 0.005838493816554546\n",
      "259/290. Loss: 0.010217731818556786\n",
      "260/290. Loss: 0.0056368461810052395\n",
      "261/290. Loss: 0.005654202774167061\n",
      "262/290. Loss: 0.005688936915248632\n",
      "263/290. Loss: 0.005489116534590721\n",
      "264/290. Loss: 0.007171188946813345\n",
      "265/290. Loss: 0.014408113434910774\n",
      "266/290. Loss: 0.006088779307901859\n",
      "267/290. Loss: 0.020528623834252357\n",
      "268/290. Loss: 0.0147323003038764\n",
      "269/290. Loss: 0.00612092949450016\n",
      "270/290. Loss: 0.005030851345509291\n",
      "271/290. Loss: 0.0068164183758199215\n",
      "272/290. Loss: 0.005667300429195166\n",
      "273/290. Loss: 0.014618702232837677\n",
      "274/290. Loss: 0.03560197353363037\n",
      "275/290. Loss: 0.014654485508799553\n",
      "276/290. Loss: 0.008746045641601086\n",
      "277/290. Loss: 0.005912879016250372\n",
      "278/290. Loss: 0.020412828773260117\n",
      "279/290. Loss: 0.005748183466494083\n",
      "280/290. Loss: 0.005917472764849663\n",
      "281/290. Loss: 0.06156470999121666\n",
      "282/290. Loss: 0.07306758314371109\n",
      "283/290. Loss: 0.006628725677728653\n",
      "284/290. Loss: 0.00791286863386631\n",
      "285/290. Loss: 0.005714841652661562\n",
      "286/290. Loss: 0.007434668485075235\n",
      "287/290. Loss: 0.0061308699660003185\n",
      "288/290. Loss: 0.016262942925095558\n",
      "289/290. Loss: 0.009391825646162033\n",
      "290/290. Loss: 0.009816806763410568\n"
     ]
    }
   ],
   "source": [
    "progress = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f'{progress}/{num_training_steps}. Loss: {loss}')\n",
    "        progress += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.75k/5.75k [00:00<00:00, 14.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8382352941176471, 'f1': 0.8862068965517241}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('glue', 'mrpc')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in val_dataloder:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
